---
title: "SMF_Project"
date: "2023-04-14"
output: html_document
---

```{r}
library(quantmod)
library(dplyr)
library(tidyquant)
library(xts)
```
## Sections 1 & 2

## Picking the 15 Assets 

```{r}
# Should be a nice selection of assets to start with... 
tickers <- c("AAPL", "MSFT", "AMZN", "GOOGL", "META", "TSLA", "BRK-B", "JNJ", "JPM", "V", "PG", "UNH", "MA", "NVDA", "DIS")

```

```{r}
get_monthly_prices_and_returns <- function(ticker) {
  prices <- getSymbols(ticker, src = "yahoo", from = "2015-01-01", to = "2021-12-31", auto.assign = FALSE)
  adjusted_prices <- Ad(prices)
  monthly_prices <- to.monthly(adjusted_prices, OHLC = FALSE, indexAt = "lastof")

  # Calculate monthly returns
  returns <- na.omit(periodReturn(monthly_prices, period = "monthly", type = "arithmetic"))

  # Convert to data frame
  prices_df <- data.frame(date = index(monthly_prices), price = coredata(monthly_prices))
  colnames(prices_df) <- c("date", paste0(ticker, "_price"))

  returns_df <- data.frame(date = index(returns), return = coredata(returns))
  colnames(returns_df) <- c("date", paste0(ticker, "_return"))

  return(list(prices = prices_df, returns = returns_df))
}

```

```{r}
prices_and_returns_list <- lapply(tickers, get_monthly_prices_and_returns)
combined_prices <- Reduce(function(x, y) merge(x, y, by = "date", all = TRUE), lapply(prices_and_returns_list, function(x) x$prices))
combined_returns <- Reduce(function(x, y) merge(x, y, by = "date", all = TRUE), lapply(prices_and_returns_list, function(x) x$returns))
```


```{r}
head(combined_returns)
combined_returns <- combined_returns[-1, ]
head(combined_returns)
head(combined_prices) # no need to remove first row 
```

```{r}
# Convert risk free to monthly from annual 
risk_free_rate<-rep_len(0.0094/12, length.out = length(combined_returns$date))

final_data <- cbind(combined_returns, risk_free_rate)

head(final_data)

```
- You should also plot your monthly prices and returns and comment on these plots.
```{r}

# Create separate plots with both prices and returns for each asset
for (ticker in tickers) {
  # Set up the layout for the two plots
  par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
  
  # Get the monthly prices and returns for the current asset
  prices <- combined_prices[, paste0(ticker, "_price")]
  returns <- combined_returns[, paste0(ticker, "_return")]
  
  # Plot the monthly prices
  plot(prices, main = paste(ticker, "Monthly Prices"), ylab = "Price", type ='l')
  points(prices)
  
  # Plot the monthly returns
  plot(returns, main = paste(ticker, "Monthly Returns"), ylab = "Return", type='l')
  points(returns)
}

```

```{r}
library(moments)
```
## Sample Statistics 
- Recall kurtosis & skewnness are indicative of normality ... 
- Also, note that using S&P 500 as market index to find excess returns for market and market variance [check if this is correct!]
```{r}

# Fetch S&P 500 data
data_for_market_index<-getSymbols("^GSPC", src = "yahoo", from = "2015-01-01", to = "2021-12-31", auto.assign = FALSE)

# Calculate the S&P 500 monthly returns
sp500_prices <- data_for_market_index$GSPC.Adjusted
sp500_monthly_prices <- to.monthly(sp500_prices, OHLC = FALSE, indexAt = "lastof")
sp500_monthly_returns <- na.omit(periodReturn(sp500_monthly_prices, period = "monthly", type = "arithmetic"))

# Convert to data frame
sp500_returns_df <- data.frame(date = index(sp500_monthly_returns), return = coredata(sp500_monthly_returns))
colnames(sp500_returns_df) <- c("date", "SP500")

# Merge the S&P 500 returns with the combined_returns data frame
combined_returns <- merge(combined_returns, sp500_returns_df, by = "date", all = TRUE)
head(combined_returns)
combined_returns <- combined_returns[-1, ]
head(combined_returns)

excess_returns <- combined_returns
for (i in 2:(ncol(combined_returns) - 1)) {
  excess_returns[, i] <- combined_returns[, i] - final_data$risk_free_rate
}
market_excess_returns <- combined_returns$SP500 - final_data$risk_free_rate

get_sample_statistics <- function(asset_excess_returns) {
  # Filter out rows with missing data
  complete_data <- na.omit(data.frame(asset_excess_returns, market_excess_returns))
  
  asset_mean <- mean(complete_data$asset_excess_returns, na.rm = TRUE) * 12
  asset_sd <- sd(complete_data$asset_excess_returns, na.rm = TRUE) * sqrt(12)
  asset_skewness <- moments::skewness(complete_data$asset_excess_returns, na.rm = TRUE)
  asset_kurtosis <- moments::kurtosis(complete_data$asset_excess_returns, na.rm = TRUE)

  # Calculate beta using the cov() and var() functions
  asset_covariance <- cov(complete_data$asset_excess_returns, complete_data$market_excess_returns, use = "complete.obs")
  market_variance <- var(complete_data$market_excess_returns, na.rm = TRUE)
  asset_beta <- asset_covariance / market_variance

  return(c(asset_mean, asset_sd, asset_skewness, asset_kurtosis, asset_beta))
}

# Calculate the statistics for the market index
market_statistics <- get_sample_statistics(combined_returns$SP500)
paste0("Market annualised summary statistics = (mean, sd, skewness, kurtosis, beta) = "); market_statistics


asset_excess_returns <- excess_returns[, 2:(ncol(excess_returns) - 1)]
# Calculate the statistics for the assets
asset_returns<-combined_returns[,2:16]
statistics_list <- lapply(asset_returns, get_sample_statistics)

statistics_df <- data.frame(matrix(unlist(statistics_list), nrow = 5, byrow = FALSE))
rownames(statistics_df) <- c("Annual Mean", "Annual SD", "Skewness", "Kurtosis", "Beta")
colnames(statistics_df) <- tickers

# Add the market statistics to the data frame
statistics_df <- cbind(statistics_df, data.frame(market_statistics))
colnames(statistics_df)[ncol(statistics_df)] <- "Market"

head(statistics_df)

```

- Summary Stat Plots: 
```{r}
get_summary_plots<- function(asset_returns, asset_name){
  h<-hist(asset_returns, main = paste0("Histogram for ", asset_name))
  b<-boxplot(asset_returns, main = paste0("Boxplot for ", asset_name))
  qq<-qqnorm(asset_returns, main = paste0("QQ-plot for ", asset_name))
}

mapply(get_summary_plots, asset_returns, tickers)
```

- You should also provide an equity curve for each asset (that is, a curve that shows the growth of a $1 in each of the asset over the time period you chose). You should do the same for S&P 500 and compare it with the assets. [Get S&P 500 data from yfinance also]
```{r}
# Calculate cumulative returns for each asset and the S&P 500
cumulative_returns <- function(asset_returns) {
  cum_returns <- cumprod(1 + asset_returns) - 1
  return(cum_returns)
}

equity_curves <- lapply(asset_returns, cumulative_returns)

# Add the S&P 500 equity curve
sp500_returns <- sp500_returns_df$SP500
sp500_equity_curve <- cumulative_returns(sp500_returns)

# Plot the equity curves
par(mfrow = c(4, 4), mar = c(4, 4, 2, 1))

for (i in 1:length(tickers)) {
  plot(equity_curves[[i]], main = paste(tickers[i], "Equity Curve"), ylab = "Cumulative Return", xlab = "Date", type = "l")
}

# Plot the S&P 500 equity curve
plot(sp500_equity_curve, main = "S&P 500 Equity Curve", ylab = "Cumulative Return", xlab = "Date", type = "l")


```

- Run a test for stationarity.

```{r}
library(tseries)

# Consider differencing? Some of these do it automatically... 
p_vals_adf<-c()
number<-ncol(combined_returns)-1
for(i in 2:number){
  col<-colnames(combined_returns)[i]
  slice<-combined_returns[colnames(combined_returns)==col]
  tesT<-adf.test(slice[[1]], alternative="stationary")
  p_vals_adf[i]<-tesT$p.value
}
p_vals_adf # Small p-values means rejecting H_0 in favour of H_a that series are all stationary 


p_vals_kpss<-c()
for(i in 2:number){
  col<-colnames(combined_returns)[i]
  slice<-combined_returns[colnames(combined_returns)==col]
  tesT<-kpss.test(slice[[1]], null="Level")
  p_vals_kpss[i]<-tesT$p.value
}
p_vals_kpss # Big p-values means keeping H_0 that series are trend stationary [look into 6th asset - Tesla]

# Both tests conclude that stationary ?

```


- Do the returns look normally distributed ? 
```{r}
library("nortest")
p_vals_lillie<-c()
for(i in 2:number){
  col<-colnames(combined_returns)[i]
  slice<-combined_returns[colnames(combined_returns)==col]
  tesT<-lillie.test(slice[[1]])
  p_vals_lillie[i]<-tesT$p.value
}
p_vals_lillie # H_0 is that data is normal so if smaller than threshold p-value, look into that asset being differently distributed 
```

- Compute Sharpeâ€™s slope for each asset. Which asset has the highest slope? 
```{r}
# Calculate Sharpe's slope for each asset
# Note (!) using monthly excess returns
sharpe_slope <- sapply(asset_excess_returns, function(x) mean(x, na.rm = TRUE) / sd(x, na.rm = TRUE))

# Print Sharpe's slope for each asset
sharpe_slope

# Find the asset with the highest Sharpe's slope
highest_sharpe_slope <- tickers[which.max(sharpe_slope)]
highest_sharpe_slope

```
- Are there any outliers in the data? [Test for this... ]
```{r}
library(tsoutliers)

outliers_list <- list()
total_outliers <- list()

for (i in 1:length(tickers)) {
  asset_ts <- ts(asset_returns[, i], start = c(2015, 1), frequency = 12)
  outliers_list[[tickers[i]]] <- tso(asset_ts, maxit = 100)
  total_outliers[[tickers[i]]] <- length(outliers_list[[tickers[i]]]$outliers$ind)
  
  # Extract the index values of the identified outliers for the current asset
  outlier_indices <- outliers_list[[tickers[i]]]$outliers$ind
  
  # Create a plot of the current asset's returns and mark the outlier points in red
  plot(x=1:length(asset_ts), y=asset_ts, main = tickers[i],type = 'l', ylab = "Returns", xlab = "Time")
  points(x=outlier_indices, y=asset_ts[outlier_indices], col = "red")
}

# Print the total number of outliers for each asset's time series
total_outliers_df <- data.frame(matrix(unlist(total_outliers), nrow = 1, byrow = FALSE))
colnames(total_outliers_df) <- c(tickers)
rownames(total_outliers_df) <- c("Total Outliers")

total_outliers_df
# By default: "AO" additive outliers, "LS" level shifts, and "TC" temporary changes are selected above ...

# 1) Additive outliers: observations significantly larger or smaller than the surrounding data points. Usually caused by random events or measurement errors.

# 2) Level shifts: abrupt changes in the level of the time series that persist over time. Often caused by sudden changes in the underlying process that generates the data.

# 3) Temporary changes are sudden, but short-lived changes in the level of the time series. Often caused by transient events that have a temporary impact on the underlying process. 
```

- Construct pairwise scatter plots between your assets returns and comment on
any relationships you see.
- Compute the sample covariance matrix of the returns on your assets and comment on the direction of linear association between the asset returns.
- Which assets are most highly correlated? Which are least correlated? 
- Based on the estimated correlation values do you think diversification will reduce risk with these assets?
```{r}
# Pairwise scatter plots
pairs(asset_returns, 
      main = "Pairwise Scatter Plots of Asset Returns",
      lower.panel = NULL)

# Compute the sample covariance matrix
cov_matrix <- cov(asset_returns, use = "complete.obs")

# Print the covariance matrix
cov_matrix

# Compute the correlation matrix
cor_matrix <- cor(asset_returns, use = "complete.obs")

# Print the correlation matrix
cor_matrix


library(gplots)

# Plot the heatmap
heatmap(cor_matrix, 
        symm = TRUE, 
        margins = c(5, 5),
        main = "Correlation Matrix of Asset Returns",
        xlab = "Asset Tickers", 
        ylab = "Asset Tickers",
        Colv = NA, Rowv = NA)


library(caret)

# Identify strongly correlated pairs (correlation coefficient > 0.6)
highly_correlated_pairs <- findCorrelation(cor_matrix, cutoff = 0.6, verbose = TRUE)

```

- Fit different distributions to your data, see which one fits better. 
 - Ask professor if can focus on assets that fail normality check only and fit normal dist. with sample(mean) and sample(sd) for the ones that should be normal? 
```{r}
library(MASS)
library(fGarch)

n_cols <- ncol(combined_returns)
best_fits_AIC <- list()
best_fits_BIC <- list()

# combined_returns[[1]]

for (i in 2:n_cols) {
  fit_norm <- fitdistr(combined_returns[[i]], "normal")
  
  Y <- combined_returns[[i]]
  loglik_sstd = function(x){ 
    f = -sum(dsstd(Y, x[1], x[2], x[3], x[4], log = TRUE)) 
    f}
  start = c(mean(Y), sd(Y), length(Y), 1)
  fit_sstd = optim(start, loglik_sstd, method = "L-BFGS-B", lower = c(-1, 0.001, 1, 0.1), upper = c(1, 10, 50, 10), hessian = TRUE, control=list(maxit=1000000, tmax=100))

  minus_logL_sstd = fit_sstd$value # minus the log-likelihood
  AIC_sstd = 2 * minus_logL_sstd + 2 * length(fit_sstd$par)
  BIC_sstd = 2 * minus_logL_sstd + log(length(Y))*length(fit_sstd$par)

  loglik_ged = function(beta) sum( - dged(Y,mean=beta[1], sd=beta[2],nu=beta[3],log=TRUE) )
  start = c(mean(Y),sd(Y),1)
  fit_ged = optim(start,loglik_ged,hessian=T,method="L-BFGS-B")                                   
  AIC_ged = 2*fit_ged$value+2*length(fit_sstd$par)
  BIC_ged = 2*fit_ged$value+log(length(Y))*length(fit_sstd$par)
  
  # Compare models using AIC and select the model with the lowest AIC
  AIC_norm <- AIC(fit_norm)
  
  AIC_values <- c(AIC_norm, AIC_sstd, AIC_ged)
  names(AIC_values) <- c("normal", "sstd", "ged")
  best_fit_AIC <- c(fit_norm, fit_sstd, fit_ged)[which.min(AIC_values)]
  best_aic <- which.min(AIC_values)
  # Compare models using BIC and select the model with the lowest BIC
  BIC_norm <- BIC(fit_norm)

  BIC_values <- c(BIC_norm, BIC_sstd, BIC_ged)
  names(BIC_values) <- c("normal", "sstd", "ged")
  best_fit_BIC <- c(fit_norm, fit_sstd, fit_ged)[which.min(BIC_values)]
  best_bic <- which.min(BIC_values)
  
  best_fits_AIC[[i]] <- list(AIC_values, best_aic, best_fit_AIC)
  best_fits_BIC[[i]] <- list(BIC_values, best_bic, best_fit_BIC)
}

```
- Results of fitting distributions: 
```{r}
for (i in 2:n_cols) {
  cat("\nAsset", i, "Results:\n")
  
  # Access the results for the asset i based on AIC
  asset_AIC_values <- best_fits_AIC[[i]][[1]]
  asset_best_aic <- best_fits_AIC[[i]][[2]]
  asset_best_fit_AIC <- best_fits_AIC[[i]][[3]]

  # Access the results for the asset i based on BIC
  asset_BIC_values <- best_fits_BIC[[i]][[1]]
  asset_best_bic <- best_fits_BIC[[i]][[2]]
  asset_best_fit_BIC <- best_fits_BIC[[i]][[3]]

  # Print AIC results
  cat("\nAIC Results:\n")
  cat("AIC Values:", asset_AIC_values, "\n")
  cat("Best Model (AIC):", names(asset_AIC_values)[asset_best_aic], "\n")
  
  # Print BIC results
  cat("\nBIC Results:\n")
  cat("BIC Values:", asset_BIC_values, "\n")
  cat("Best Model (BIC):", names(asset_BIC_values)[asset_best_bic], "\n")
}

```
## Section 3 PORTFOLIO THEORY
Necessary libraries:
```{r}
# install.packages('IntroCompFinR')
library(quadprog)
library(PortfolioAnalytics)
library(tidyverse)
# library(IntroCompFinR)
library(fPortfolio)
library(reshape2)
```

Necessary objects and values from the assets:
```{r}
asset_returns_mat <-as.matrix(asset_returns)

#calculate Annualized expected returns and variances for each asset
ann_asset_exp_returns <- colMeans(asset_returns_mat) * 12
ann_asset_risks <- apply(asset_returns_mat, 2, sd) * sqrt(12)

# for calc of VaR: creating the returns matrix in a form of (1+r)
asset_monthly_returns <- apply(asset_returns_mat, 2, function(x) x + 1)
asset_monthly_returns <- apply(asset_monthly_returns, 2, function(x) prod(x)^(1/length(x))-1)

asset_sd <- apply(asset_returns_mat, 2, sd) #the vector of SDs for each asset's monthly returns
asset_var_5 <- qnorm(0.05, mean = asset_monthly_returns, sd = asset_sd) # vector of VaRs for each asset (based on monthly returns)
asset_var_5

#for efficient frontier:
asset_means <-colMeans(asset_returns) #mean of monthly returns for each asset

```



Compute the minimum variance portfolio (MVP) WITHOUT short sales:
```{r}
n <- ncol(asset_returns_mat)
Dmat <- cov_matrix
dvec <- rep(0, n)
Amat <- rbind(rep(1, n), diag(n))
bvec <- c(1, rep(0, n))
sol <- solve.QP(Dmat = Dmat, dvec = dvec, Amat = t(Amat), bvec = bvec, meq = 1)
mvp_w <- sol$solution
#sum(mvp_w) #sense check
cat("Minimum Variance Portfolio WITHOUT short sales:\n")
cat(paste(round(mvp_w, 4), collapse = ", "))

gmin.port = globalMin.portfolio(asset_means, cov_matrix, shorts = FALSE)
gmin.port

```

Compute the minimum variance portfolio (MVP) with shorting allowed:
```{r MVP}
# SHORT SALES ALLOWED->..._ss
# Define the objective function and constraints for the quadratic optimization problem
# Set up the constraint matrix Amat and the constraint vector bvec

Dmat <- as.matrix(cov_matrix)
dvec_ss <- rep(0, n)
Amat_ss <- cbind(rep(1, n), diag(n), -diag(n))
bvec_ss <- c(1, rep(-0.1, n), rep(-0.5, n))
sol_ss <- solve.QP(Dmat = Dmat, dvec = dvec_ss, Amat = Amat_ss, bvec = bvec_ss, meq = 1)

mvp_w_ss <- sol_ss$solution

#sum(mvp_w_ss) #sense check
cat("Minimum Variance Portfolio WITH short sales:\n")
cat(paste(round(mvp_w_ss, 4), collapse = ", "))

gmin_port = globalMin.portfolio(asset_means, cov_matrix, shorts = TRUE)
gmin_port
```

```{r}
# Calculate the MVP mean return and standard deviation
mvp_mean <- mean(asset_returns_mat %*% mvp_w)
mvp_sd <- sqrt(t(mvp_w) %*% cov_matrix %*% mvp_w)

# Calculate the MVP value at risk (VaR)
alpha <- 0.05 # Set the significance level
mvp_var <- quantile(asset_returns_mat %*% mvp_w, alpha)

# Calculate the MVP expected shortfall (ES)
mvp_es <- mean(asset_returns_mat[asset_returns_mat %*% mvp_w < mvp_var, ] %*% mvp_w)

#Annualizing the monthly mean and risk by multiplying the mean and the risk by 12.
annual_mvp_mean <- 12 * mvp_mean
annual_mvp_sd <- sqrt(12) * mvp_sd
annual_mvp_mean
annual_mvp_sd
```




Estimate its mean return, its standard deviation, its value at risk and expected shortfall.
```{r}
# Calculate the MVP mean return and standard deviation
mvp_mean_ss <- mean(asset_returns_mat %*% mvp_w_ss)
mvp_sd_ss <- sqrt(t(mvp_w_ss) %*% cov_matrix %*% mvp_w_ss)

# Calculate the MVP value at risk (VaR)
alpha <- 0.05 # Set the significance level
mvp_var_ss <- quantile(asset_returns_mat %*% mvp_w_ss, alpha)

# Calculate the MVP expected shortfall (ES)
mvp_es_ss <- mean(asset_returns_mat[asset_returns_mat %*% mvp_w_ss < mvp_var_ss, ] %*% mvp_w_ss)

#Annualizing the monthly mean and risk by multiplying the mean and the risk by 12.

annual_mvp_mean_ss <- 12 * mvp_mean_ss
annual_mvp_sd_ss <- sqrt(12) * mvp_sd_ss
annual_mvp_mean_ss
annual_mvp_sd_ss

```

Comment on the weights of MVP:
```{r}
mvp_t <- cbind(colnames(asset_returns), round(mvp_w,3), round(mvp_w_ss,3), round(asset_sd,3))
colnames(mvp_t) <- c("Asset", "MVP weights without shorting","MVP weights with shorting","Asset risk")
mvp_t

#annualized measures table, VaRs and ES comparison
mvp_m <-c(round(annual_mvp_mean, 3), round(annual_mvp_sd, 3), round(mvp_var, 3), round(mvp_es, 3))
mvp_m_ss <- c(round(annual_mvp_mean_ss, 3), round(annual_mvp_sd_ss, 3), round(mvp_var_ss, 3), round(mvp_es_ss, 3))

mvp_mt <-cbind(mvp_m, mvp_m_ss)
colnames(mvp_mt) <- c("MVP w/o shorting","MVP w/ shorting")
rownames(mvp_mt) <- c( "Annual Mean Return", "Annual Standard Deviation", "VaR(5%, t=1month)", "ES(5%, t=1month)" )

mvp_mt
```
When short positions is NOT allowed:
The portfolio is heavily weighted on PG stocks with w=0.458, which seems to be among less volatile stocks. This is in line with minimization of the variance of the portfolio. Soem of the stocks have weights close to zero. This can happen if these assets have high correlations with other assets in the portfolio, or if they have high individual variances that make them unattractive from a risk/return perspective.

When short positions is allowed:
Based on the weights printed in the output, we can see that the MVP is heavily weighted towards PG again, with a weight of 0.496. Since the MVP is designed to minimize portfolio variance, we still want to hold long position with a less volatile asset. Likewise, assets with higher volatility are shorted, but still with weights less than 10%.

```{r}
# Compare each asset's annual expected return to the annual MVP mean
higher_than_mvp_mean <- ann_asset_exp_returns > annual_mvp_mean
lower_than_mvp_mean <- ann_asset_exp_returns < annual_mvp_mean
same_as_mvp_mean <- ann_asset_exp_returns == annual_mvp_mean

# Construct a data frame to summarize the results
comparison_mean <- data.frame(
  Expected_Return = round(ann_asset_exp_returns,3),
  MVP_Mean = round(annual_mvp_mean,3),
  Higher_Than_MVP = ifelse(higher_than_mvp_mean, "Yes", "No"),
  #Lower_Than_MVP = ifelse(lower_than_mvp_mean, "Yes", "No"),
  Same_As_MVP = ifelse(same_as_mvp_mean, "Yes", "No")
)

# Create a data frame to compare each asset's risk to the MVP's risk
higher_than_mvp_sd <- sapply(ann_asset_risks, function(x) x > annual_mvp_sd)
lower_than_mvp_sd <- sapply(ann_asset_risks, function(x) x < annual_mvp_sd)
same_as_mvp_sd <- sapply(ann_asset_risks, function(x) x == annual_mvp_sd)

comparison_sd <- data.frame(
  Asset_Risk = round(ann_asset_risks,3),
  MVP_Risk = round(annual_mvp_sd,3),
  Higher_Than_MVP = ifelse(higher_than_mvp_sd, "Yes", "No"),
  #Lower_Than_MVP = ifelse(lower_than_mvp_sd , "Yes", "No"),
  Same_As_MVP = ifelse(same_as_mvp_sd, "Yes", "No")
)

print(comparison_mean)
print(comparison_sd)
```


```{r}
# Compare each asset's annual expected return to the annual MVP mean
higher_than_mvp_mean_s <- ann_asset_exp_returns > annual_mvp_mean_ss
lower_than_mvp_mean_s <- ann_asset_exp_returns < annual_mvp_mean_ss
same_as_mvp_mean_s <- ann_asset_exp_returns == annual_mvp_mean_ss

# Construct a data frame to summarize the results
comparison_mean_ss <- data.frame(
  Expected_Return = round(ann_asset_exp_returns,3),
  MVP_Mean_ss = round(annual_mvp_mean_ss,3),
  Higher_Than_MVP = ifelse(higher_than_mvp_mean_s, "Yes", "No"),
  #Lower_Than_MVP = ifelse(lower_than_mvp_mean_s, "Yes", "No"),
  Same_As_MVP = ifelse(same_as_mvp_mean_s, "Yes", "No")
)

# Create a data frame to compare each asset's risk to the MVP's risk
higher_than_mvp_sd_s <- sapply(ann_asset_risks, function(x) x > annual_mvp_sd_ss)
lower_than_mvp_sd_s <- sapply(ann_asset_risks, function(x) x < annual_mvp_sd_ss)
same_as_mvp_sd_s <- sapply(ann_asset_risks, function(x) x == annual_mvp_sd_ss)

comparison_sd_ss <- data.frame(
  Asset_Risk = round(ann_asset_risks,3),
  MVP_Risk = round(annual_mvp_sd_ss,3),
  Higher_Than_MVP = ifelse(higher_than_mvp_sd_s, "Yes", "No"),
  #Lower_Than_MVP = ifelse(lower_than_mvp_sd_s , "Yes", "No"),
  Same_As_MVP = ifelse(same_as_mvp_sd_s, "Yes", "No")
)

print(comparison_mean_ss)
print(comparison_sd_ss)
```
The comment below is applciable for both cases: with and without short sales due to the difference between MVP mean return values being small:
From the output above you can see that MVP's annualized risk is lower than that of each of individual asset. This confirms that diversification by including multiple assets into a portfolio lowers risks of a portfolio. However, comparing the annual expected returns of the assets to the MVP's annualized mean return we can see that 11 out of 15 assets have higher individual returns than the MVP mean. This confirms that minimizing the risk does not necessarily lead to the maximizing the expected return. 




Assume that you have USD100,000 to invest. For the MVP, determine the 5% value-at-risk of the USD100,000 investment over a one month investment horizon.
```{r}
var_amount <- 100000*mvp_var
var_amount

var_amount_ss <- 100000*mvp_var_ss
var_amount_ss
```

Compare this value to the VaR values for the individual assets.

```{r}
#without shorting
higher_than_mvp_var <- sapply(asset_var_5, function(x) abs(x) > abs(mvp_var))
lower_than_mvp_var <- sapply(asset_var_5, function(x) abs(x) < abs(mvp_var))
same_as_mvp_var <- sapply(asset_var_5, function(x) abs(x) == abs(mvp_var))

comparison_VaR <- data.frame(
  Asset_VaR = round(asset_var_5,3),
  MVP_VaR = round(mvp_var,3),
  Higher_Than_MVP = ifelse(higher_than_mvp_var, "Yes", "No"),
  #Lower_Than_MVP = ifelse(lower_than_mvp_var, "Yes", "No"),
  Same_As_MVP = ifelse(same_as_mvp_var, "Yes", "No")
)

print(comparison_VaR)
```

```{r}
#with shorting
higher_than_mvp_var_s <- sapply(asset_var_5, function(x) abs(x) > abs(mvp_var_ss))
lower_than_mvp_var_s <- sapply(asset_var_5, function(x) abs(x) < abs(mvp_var_ss))
same_as_mvp_var_s <- sapply(asset_var_5, function(x) abs(x) == abs(mvp_var_ss))

comparison_VaR_s <- data.frame(
  Asset_VaR = round(asset_var_5,4),
  MVP_VaR_s = round(mvp_var_ss,4),
  Higher_Than_MVP = ifelse(higher_than_mvp_var_s, "Yes", "No"),
  #Lower_Than_MVP = ifelse(lower_than_mvp_var_s, "Yes", "No"),
  Same_As_MVP = ifelse(same_as_mvp_var_s, "Yes", "No")
)

print(comparison_VaR_s)
```
From the output above (comparing the absolute values of VaRs) we can see that the Value-at-Risk of each individual asset is higher than that of MVP. This demonstrates that MVP does a good job diversifying the portfolio thus decreasing the risk for the investor. 





Using the estimated means, variances and covariances computed earlier, compute the efficient portfolio frontier, with short sales NOT allowed
```{r}
tan_port = tangency.portfolio(asset_means, cov_matrix, 0.0094/12, shorts= FALSE)
tan_port
tan_port_w<- tan_port$weights
ef = efficient.frontier(asset_means, cov_matrix, alpha.min=-0.5, 
                        alpha.max=2, nport=70, shorts = FALSE)

plot(ef, plot.assets=F, col="grey", pch=16) #if assets included => plot.assets =TRUE
points(mvp_sd, mvp_mean, col="green", pch=16, cex=2) 
text(mvp_sd, mvp_mean, labels = "MVP", pos = 4) 

points(tan_port$sd, tan_port$er, col="red", pch=16, cex=2) 
text(tan_port$sd, tan_port$er, labels = "Tangency", pos = 3) 

sr_tan = (tan_port$er - 0.0094/12)/tan_port$sd 
abline(a=0.0094/12, b=sr_tan, col="purple", lwd=2)
round(sr_tan,3)

```

Compute the tangency portfolio when short-sales are not allowed and compute its expected return and standard deviation
```{r}
tan_port_ss = tangency.portfolio(asset_means, cov_matrix, 0.0094/12, shorts= T)
tan_port_w_ss<- tan_port_ss$weights
ef_ss = efficient.frontier(asset_means, cov_matrix, alpha.min=-0.5, 
                        alpha.max=2, nport=70, shorts = T)


plot(ef_ss, plot.assets=F, col="blue", pch=16) #if assets included => plot.assets =TRUE
points(mvp_sd_ss, mvp_mean_ss, col="green", pch=16, cex=2) 
text(mvp_sd_ss, mvp_mean_ss, labels = "MVP", pos = 4) 

points(tan_port_ss$sd, tan_port_ss$er, col="red", pch=16, cex=2) 
text(tan_port_ss$sd, tan_port_ss$er, labels = "Tangency", pos = 3) 

sr_tan_ss = (tan_port_ss$er - 0.0094/12)/tan_port_ss$sd 
abline(a=0.0094/12, b=sr_tan_ss, col="purple", lwd=2)
round(sr_tan_ss,3)
```
For the risky assets using the Markowitz approach Compare the Sharpe ratios of each asset with that of the tangency portfolio.[NO SHORTING]

```{r}
#NO SHORTING
#without shorting
higher_than_tan <- sapply(sharpe_slope, function(x) x > sr_tan)
lower_than_tan <- sapply(sharpe_slope, function(x) x < sr_tan)
same_as_tan <- sapply(sharpe_slope, function(x) x == sr_tan)

comparison_SR <- data.frame(
  Asset_SR = round(sharpe_slope,4),
  Tan_SR = round(sr_tan,4),
  Higher_Than_Tan = ifelse(higher_than_tan, "Yes", "No")
  #Lower_Than_Tan = ifelse(lower_than_tan, "Yes", "No"),
  #Same_As_Tan = ifelse(same_as_tan, "Yes", "No")
)

print(comparison_SR)


#with shorting
higher_than_tan_ss <- sapply(sharpe_slope, function(x) x > sr_tan_ss)
lower_than_tan_ss <- sapply(sharpe_slope, function(x) x < sr_tan_ss)
same_as_tan_ss <- sapply(sharpe_slope, function(x) x == sr_tan_ss)

comparison_SR_ss <- data.frame(
  Asset_SR_ss = round(sharpe_slope,4),
  Tan_SR_ss = round(sr_tan_ss,4),
  Higher_Than_Tan_ss = ifelse(higher_than_tan_ss, "Yes", "No")
  #Lower_Than_Tan_ss = ifelse(lower_than_tan_ss, "Yes", "No"),
  #Same_As_Tan_ss = ifelse(same_as_tan_ss, "Yes", "No")
)

print(comparison_SR_ss)


```
```{r}
tp_w<- cbind( round(tan_port_w,3), round(tan_port_w_ss,3))
tp_w
```


```{r}
#tangency portfolio measurements
tp_m <-rbind(c(round(tan_port$er,3), round(tan_port$sd,3)),
             c(round(tan_port_ss$er,3),round(tan_port_ss$sd,3)))

rownames(tp_m) <-c("Without Shorting", "With Shorting")

tp_mf <- cbind(tp_m,c(round(sr_tan,3),round(sr_tan_ss,3)))
colnames(tp_mf) <-c("Expected returns", "StDev", "Sharpe ratio")
tp_mf
```


## Sections 4 Asset Allocation 
Suppose you wanted to achieve a target expected return of 6% per year (which corresponds
to an expected return of 0.5% per month) using only the risky assets and no short sales
allowed, what is the efficient portfolio that achieves this target return? How much is invested in each of the assets in this efficient portfolio? 
Compute the monthly risk on this efficient portfolio, as well as the monthly 5% value-at-risk and expected shortfall based on an initial $100,000 investment. 

```{r}
# Install DEoptim package if not already installed
if (!requireNamespace("DEoptim", quietly = TRUE)) {
  install.packages("DEoptim")
}

# Load the DEoptim package
library(DEoptim)

```

```{r}
# Define the number of risky assets
n_assets <- ncol(asset_returns)

# Define the target monthly return
target_return <- 0.005

# Define the covariance matrix of the asset returns
cov_matrix <- cov(asset_returns)

# Define the vector of expected returns of the risky assets
expected_returns <- apply(asset_returns, 2, mean)

# Define the optimization inputs
Dmat <- as.matrix(cov_matrix)
dvec <- rep(0, n_assets)
Amat <- rbind(rep(1, n_assets), expected_returns)
bvec <- c(1, target_return)

# Define additional constraints for no short selling
Amat <- rbind(Amat, diag(n_assets))
bvec <- c(bvec, rep(0, n_assets))

# Solve the optimization problem
solution <- solve.QP(Dmat, dvec, t(Amat), bvec, meq=1)

# Extract the optimal weights from the solution
optimal_weights <- solution$solution

# Fix the negative weights
optimal_weights[optimal_weights < 0] <- 0

# Normalize the weights to sum to 1
optimal_weights <- optimal_weights/sum(optimal_weights)

# Print the optimal weights
print(optimal_weights)

# Compute the monthly risk of the efficient portfolio
efficient_portfolio_risk <- sqrt(t(optimal_weights) %*% cov_matrix %*% optimal_weights)
print(efficient_portfolio_risk)

# Compute the monthly 5% value-at-risk of the efficient portfolio
portfolio_value <- 100000
portfolio_returns <- as.matrix(asset_returns) %*% optimal_weights
portfolio_value_at_risk <- -quantile(portfolio_returns, 0.05) * portfolio_value
paste0("Portfolio VaR is: ", portfolio_value_at_risk)

# Compute the monthly expected shortfall of the efficient portfolio
portfolio_expected_shortfall <- -mean(portfolio_returns[portfolio_returns <= quantile(portfolio_returns, 0.05)]) * portfolio_value
paste0("Portfolio ES is: ", portfolio_expected_shortfall)

```


Now suppose you wanted to achieve a target expected return of 6% per year (which corresponds to an expected return of 0.5% per month) using a combination of T-Bills and the tangency portfolio (that does not allow for short sales). In this allocation, how much is invested in each of the assets, and how much is invested in the risk-free asset?


```{r}
# input parameters
target_return <- 0.005 # 0.5% monthly
risk_free_rate <- 0.0094/12.0 # 0.94% yearly converted to montly
num_assets <- 15
returns_matrix <- asset_returns_mat
covariance_matrix <- cov(returns_matrix)

# calculate expected return and standard deviation of tangency portfolio
tangency_weights <- matrix(c(0,0.255,0,0,0,0.031,0,0,0,0,0.271,0.236,0,0.207,0))
tangency_return <- sum(tangency_weights * colMeans(returns_matrix))
tangency_risk <- sqrt(t(tangency_weights) %*% covariance_matrix %*% tangency_weights)

# calculate optimal allocation between risk-free asset and tangency portfolio
sigma_p <- sqrt((target_return - risk_free_rate) * (tangency_risk^2)) / (tangency_return - risk_free_rate)
weight_tangency <- sigma_p / tangency_risk
weight_risk_free <- 1 - weight_tangency
print(weight_tangency)
# calculate allocation to each asset in the efficient portfolio
for (i in 1:num_assets) {
  efficient_weights[i] = tangency_weights[i] * weight_tangency
}
#efficient_weights <- tangency_weights
efficient_weights <- c(efficient_weights, weight_risk_free)

# output results
cat("Allocation to each asset in efficient portfolio:\n")
for (i in 1:num_assets) {
  cat(sprintf("Asset %d: %.2f%%\n", i, efficient_weights[i] * 100))
}
cat(sprintf("Risk-free asset: %.2f%%\n", efficient_weights[num_assets+1] * 100))

```

Compute the monthly risk on this efficient portfolio, as well as the monthly and 5% value-at-risk and expected shortfall based on an initial $100,000 investment.

```{r}
# calculate monthly mean and covariance matrix
monthly_returns <- colMeans(returns_matrix)
monthly_covariance <- cov(returns_matrix)

# calculate monthly mean and standard deviation of efficient portfolio
monthly_efficient_return <- sum(efficient_weights[1:num_assets] * monthly_returns)
monthly_efficient_std_dev <- sqrt(t(efficient_weights[1:num_assets]) %*% monthly_covariance %*% efficient_weights[1:num_assets])

# calculate monthly value-at-risk and expected shortfall
monthly_VaR <- qnorm(0.05, mean = monthly_efficient_return, sd = monthly_efficient_std_dev) * 100000
monthly_ES <- monthly_efficient_return - (monthly_efficient_std_dev * (dnorm(qnorm(0.05)) / 0.05)) * 100000

# output results
cat(sprintf("Monthly risk of efficient portfolio: %.2f%%\n", monthly_efficient_std_dev * 100))
cat(sprintf("Monthly 5%% value-at-risk of efficient portfolio: $%.2f\n", monthly_VaR))
cat(sprintf("Monthly expected shortfall of efficient portfolio: $%.2f\n", monthly_ES))
```

## Section 5 PCA and Factor Analysis 
- Run the PCA analysis and comment on your results. [Professor mentioned to aim for 7-8 assets with 90% cumilative explained variance]
```{r}
library(stats)
asset_returns_standardized <- scale(asset_returns)
pca_results <- prcomp(asset_returns_standardized, center = FALSE, scale. = FALSE)
# Print a summary of the PCA results
summary(pca_results)

# Get the principal components
principal_components <- pca_results$x

# Get the loadings (eigenvectors)
loadings <- pca_results$rotation

# Get the proportion of variance explained by each principal component
variance_proportion <- pca_results$sdev^2 / sum(pca_results$sdev^2)

# Get the cumulative proportion of variance explained by the principal components
cumulative_variance_proportion <- cumsum(variance_proportion)

# Print the proportion of variance explained by each principal component
cat("Proportion of variance explained by each principal component:\n")
print(variance_proportion)

# Print the cumulative proportion of variance explained by the principal components
cat("Cumulative proportion of variance explained by the principal components:\n")
print(cumulative_variance_proportion)

# Find the number of components that explain at least 90% of the variance
num_components <- which(cumulative_variance_proportion >= 0.9)[1]
# Need 9 components for 90%...

# Select the first N principal components
selected_principal_components <- pca_results$x[, 1:num_components]


# Generate the reduced dataset
reduced_dataset <- selected_principal_components %*% t(pca_results$rotation[, 1:num_components])

# Add the column means back to the reduced dataset to reverse the centering
reduced_dataset <- sweep(reduced_dataset, 2, colMeans(asset_returns), "+")

# The reduced_dataset now contains original data projected onto the first N principal components, which explain at least 90% of the variance

# head(reduced_dataset)
```
- Run CAPM on the stocks 
```{r}
# Run CAPM model with S&P 500 used as the market as above... 
CAPM<-lm(as.matrix(asset_returns)~combined_returns$SP500)
CAPM
summary(CAPM)
```
- Run factor analysis and report the number and the loadings of each factors. Do they have any meaningful interpretation?
- The 'factanal()' function is a way to perform maximum-likelihood factor analysis on a data/covariance matrix. The p-value in factanal(factors=k) are testing the null hypothesis that the k-factor model fits the data well (via Chi-square statistic).
  - P-values can be used to see how well k-factor model fits the data... 
    - If this PVAL < 0.05 we reject H0 so need to test different number of factors... 
    - Can see that need 4 or more factors based on p-value and that get 77.5% cumilative explained var with 9 factors
```{r}
# Run factor analysis
two_factor_results <- factanal(asset_returns, factors=2, rotation = "varimax")
two_factor_results$loadings
two_factor_results$PVAL # PVAL << 0.05

three_factor_results <- factanal(asset_returns, factors=3, rotation = "varimax")
three_factor_results$loadings
three_factor_results$PVAL # PVAL < 0.05

four_factor_results <- factanal(asset_returns, factors=4, rotation = "varimax")
four_factor_results$loadings
four_factor_results$PVAL # PVAL > 0.05 

five_factor_results <- factanal(asset_returns, factors=5, rotation = "varimax")
five_factor_results$loadings
five_factor_results$PVAL # # PVAL > 0.05 

six_factor_results <- factanal(asset_returns, factors=6, rotation = "varimax")
six_factor_results$loadings
six_factor_results$PVAL # # PVAL > 0.05 

seven_factor_results <- factanal(asset_returns, factors=7, rotation = "varimax")
seven_factor_results$loadings
seven_factor_results$PVAL # # PVAL > 0.05 

eight_factor_results <- factanal(asset_returns, factors=8, rotation = "varimax")
eight_factor_results$loadings
eight_factor_results$PVAL # # PVAL > 0.05  

nine_factor_results <- factanal(asset_returns, factors=9, rotation = "varimax")
nine_factor_results$loadings
nine_factor_results$PVAL # # PVAL > 0.05  

ten_factor_results <- factanal(asset_returns, factors=10, rotation = "varimax")
ten_factor_results$loadings
ten_factor_results$PVAL # too many factors for 15 variables ...  
```
## Section 6 Risk Management 

```{r}
library(quantmod)
library(PerformanceAnalytics)
df = combined_returns
#set ID column as row names
rownames(df) <- df$date
#remove original ID column from data frame
df$date <- NULL
returns <- df
cat("Historical VaR is", -100000*VaR(returns, p = 0.95, method = "historical"), "\n")
cat("Historical ES is", -100000*CVaR(returns, p = 0.95, method = "historical"), "\n")
cat("Gaussian VaR is", -100000*VaR(returns, p = 0.95, method = "gaussian"), "\n")
cat("Gaussian ES is", -100000*CVaR(returns, p = 0.95, method = "gaussian"))
```

## Section 7 Copulas 

- Use copulas to to model the joint distribution of the returns. Which copula fits better the data? What are the implications? 
- Done pairwise first ... 
```{r}
library(copula)

combined_returns_matrix <- data.matrix(combined_returns[,-c(1, ncol(combined_returns))])
n_rows <- nrow(combined_returns_matrix)
n_cols <- ncol(combined_returns_matrix)

ranked_returns <- apply(combined_returns_matrix, 2, rank) / (n_rows + 1)

pairwise_fits <- list()

for (i in 1:(n_cols - 1)) {
  for (j in (i + 1):n_cols) {
    pairwise_data <- ranked_returns[, c(i, j)]
    suppressWarnings({
    # Fit Gaussian copula
    gaussian_copula <- normalCopula(param = 0, dim = 2, dispstr = "un")
    gaussian_fit <- fitCopula(gaussian_copula, pairwise_data, method = "mpl")
    AIC_gaussian <- AIC(gaussian_fit)
    BIC_gaussian <- BIC(gaussian_fit)

    # Fit t copula
    t_copula <- tCopula(param = 0, dim = 2, dispstr = "un")
    t_fit <- fitCopula(t_copula, pairwise_data, method = "mpl")
    AIC_t <- AIC(t_fit)
    BIC_t <- BIC(t_fit)

    # Fit Gumbel copula
    gumbel_copula <- gumbelCopula(dim = 2)
    gumbel_fit <- fitCopula(gumbel_copula, pairwise_data, method = "mpl")
    AIC_gumbel <- AIC(gumbel_fit)
    BIC_gumbel <- BIC(gumbel_fit)

    # Fit Clayton copula
    clayton_copula <- claytonCopula(dim = 2)
    clayton_fit <- fitCopula(clayton_copula, pairwise_data, method = "mpl")
    AIC_clayton <- AIC(clayton_fit)
    BIC_clayton <- BIC(clayton_fit)
    })
    AIC_values <- c(AIC_gaussian, AIC_t, AIC_gumbel, AIC_clayton)
    BIC_values <- c(BIC_gaussian, BIC_t, BIC_gumbel, BIC_clayton)
    names(AIC_values) <- names(BIC_values) <- c("gaussian", "t", "gumbel", "clayton")

    best_fit_AIC <- which.min(AIC_values)
    best_fit_BIC <- which.min(BIC_values)

    pairwise_fits[[paste("Asset", i, "vs", "Asset", j)]] <- list(
      AIC = AIC_values,
      BIC = BIC_values,
      best_fit_AIC = best_fit_AIC,
      best_fit_BIC = best_fit_BIC
    )
  }
}

pairwise_fits
# summary(pairwise_fits)
```
- Try to fit to all assets at once using data projected onto principal components from before 

```{r}
# Load required libraries
library(copula)
library(fGarch)
library(MASS)
library(fCopulae)

# Define the sample size n
n <- nrow(asset_returns)

# Transform the data using the empiricals
edata <- apply(asset_returns, 2, function(x) rank(x) / (n + 1))

# Fit the normal copula
ncop <- normalCopula(param = rep(0, 105), dim = 15, dispstr = "un")
fn <- fitCopula(data = edata, copula = ncop, method = "ml")

# Fit the t-copula
tcop <- tCopula(param = rep(0, 105), dim = 15, dispstr = "un", df = 5)
ft <- fitCopula(data = edata, copula = tcop, method = "ml")

# Fit the clayton copula
clcop <- archmCopula(family = "clayton", dim = 15, param = 2)
fclayton <- fitCopula(data = edata, method = "ml", copula = clcop)

# Fit the gumbel copula
gcop <- archmCopula(family = "gumbel", dim = 15, param = 2)
fgumbel <- fitCopula(data = edata, method = "ml", copula = gcop)

fn
ft
fclayton
fgumbel
```


```{r}
# Normal copula
normal_log_likelihood <- fn@loglik
normal_params <- 105
normal_aic <- -2 * normal_log_likelihood + 2 * normal_params

# T-copula
t_log_likelihood <- ft@loglik
t_params <- 105 + 1  # +1 for the degrees of freedom parameter
t_aic <- -2 * t_log_likelihood + 2 * t_params

# Clayton copula
clayton_log_likelihood <- fclayton@loglik
clayton_params <- 1
clayton_aic <- -2 * clayton_log_likelihood + 2 * clayton_params

# Gumbel copula
gumbel_log_likelihood <- fgumbel@loglik
gumbel_params <- 1
gumbel_aic <- -2 * gumbel_log_likelihood + 2 * gumbel_params

# Display the AIC values
cat("Normal copula AIC:", normal_aic, "\n")
cat("T-copula AIC:", t_aic, "\n")
cat("Clayton copula AIC:", clayton_aic, "\n")
cat("Gumbel copula AIC:", gumbel_aic, "\n")

```


